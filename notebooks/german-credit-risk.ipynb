{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Predicting Loan Risk using SparkML"}, {"metadata": {}, "cell_type": "markdown", "source": "We'll use this notebook to create a machine learning model to predict customer churn. In this notebook we will build the prediction model using the SparkML library.\n\nThis notebook walks you through these steps:\n\n- Load and Visualize data set.\n- Build a predictive model with SparkML API"}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.0 Install required packages\n\nThere are a couple of Python packages we will use in this notebook. First we make sure the Watson Machine Learning client v3 is removed (its not installed by default) and then install/upgrade the v4 version of the client.\n\nWML Client: https://wml-api-pyclient-dev-v4.mybluemix.net/#repository"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "### 1.1 Package Installation"}, {"metadata": {}, "cell_type": "code", "source": "import warnings\nwarnings.filterwarnings('ignore')", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "!pip uninstall watson-machine-learning-client -y | tail -n 1\n!pip install --user watson-machine-learning-client-v4==1.0.95 --upgrade | tail -n 1\n!pip install --user pyspark==2.3.3 --upgrade | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Package Imports"}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport json\nimport os", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.0 Load and Clean data\n\nWe'll load our data as a pandas data frame.\n\n**<font color='red'><< FOLLOW THE INSTRUCTIONS BELOW TO LOAD THE DATASET >></font>**\n\n* Highlight the cell below by clicking it.\n* In the upper right of this notebook, click on `01/00` to open the Data tab, then click on the `Connections` where you should see the Cognos Analytics connection you created earlier.\n* Click on `Insert to code` -> `pandas DataFrame`\n* The inserted code will create a Cognos Analytics data connector, which we will use to inport/export data back and forth to CA.\n* Run the cell"}, {"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# Place cursor below and use the \"Insert to code - pandas DataFrame\" option to access the CA connector# @hidden_cell\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Get the German credit data we will use to create our model\n# In order to access the data, modify the following line by changing the path and filename to match your data asset in CA.\n# NOTE: the file name should be similar to \"german_credit_model_data.csv\".\ndf = CADataConnector.read_data(path=\".public_folders/__data_folder__/__data_asset__\")\ndf.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1 Drop Some Features\nSome columns are data attributes that we will not want to use in the machine learning model. We can drop those columns / features:\n\n- CustomerID feature (column)"}, {"metadata": {}, "cell_type": "code", "source": "#Drop some columns, ignoring errors for missing keys in case we use different data sets.\ndf = df.drop(columns=['CustomerID'], axis=1, errors='ignore')\ndf.head(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2 Examine the data types of the features"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "df.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Statistics for the columns (features). Set it to all, since default is to describe just the numeric features.\ndf.describe(include = 'all')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We see that the loan amounts range from 250 to ~11,600. That the age range for applicants is between 19 and 74. etc."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3 Check for missing data\n\nWe should check if there are missing values in our dataset. There are various ways we can address this issue:\n\n- Drop records with missing values \n- Fill in the missing value with one of the following strategies: Zero, Mean of the values for the column, Random value, etc)."}, {"metadata": {}, "cell_type": "code", "source": "# Check if we have any NaN values and see which features have missing values that should be addressed\nprint(df.isnull().values.any())\ndf.isnull().sum()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Depending on which version of the dataset that you used, there may be no missing values. If there are any missing values from the output above, the sample below would be one approach to handle this issue by imputing the values for the column that reported missing data (i.e the `CURRENTRESIDENCEDURATION` column in the code as an example):\n"}, {"metadata": {}, "cell_type": "code", "source": "#from sklearn.preprocessing import Imputer\n#\n#target_idx = df.columns.get_loc(\"CurrentResidenceDuration\")\n#imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n#df.iloc[:, target_idx] = imputer.fit_transform(df.iloc[:,target_idx].values.reshape(-1, 1))\n#df.iloc[:, target_idx] = pd.Series(df.iloc[:, target_idx])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.4 Categorize Features\n\nWe will categorize some of the columns / features based on wether they are categorical values or continuous (i.e numerical) values. We will use this in later sections to build visualizations."}, {"metadata": {}, "cell_type": "code", "source": "TARGET_LABEL_COLUMN_NAME = 'Risk'\ncolumns_idx = np.s_[0:] # Slice of first row(header) with all columns.\nfirst_record_idx = np.s_[0] # Index of first record\n\nstring_fields = [type(fld) is str for fld in df.iloc[first_record_idx, columns_idx]] # All string fields\nall_features = [x for x in df.columns if x != TARGET_LABEL_COLUMN_NAME]\ncategorical_columns = list(np.array(df.columns)[columns_idx][string_fields])\ncategorical_features = [x for x in categorical_columns if x != TARGET_LABEL_COLUMN_NAME]\ncontinuous_features = [x for x in all_features if x not in categorical_features]\n\nprint('All Features: ', all_features)\nprint('\\nCategorical Features: ', categorical_features)\nprint('\\nContinuous Features: ', continuous_features)\nprint('\\nAll Categorical Columns: ', categorical_columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.5 Visualize data\n\nData visualization can be used to find patterns, detect outliers, understand distribution and more. We can use graphs such as:\n\n- Histograms, boxplots, etc: To find distribution / spread of our continuous variables.\n- Bar charts: To show frequency in categorical values.\n"}, {"metadata": {}, "cell_type": "code", "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\nsns.set_palette(\"hls\", 3)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "First, we get a high level view of the distribution of Risk. What percentage of applicants in our dataset represent Risk vs No Risk."}, {"metadata": {}, "cell_type": "code", "source": "print(df.groupby([TARGET_LABEL_COLUMN_NAME]).size())\nrisk_plot = sns.countplot(data=df, x=TARGET_LABEL_COLUMN_NAME, order=df[TARGET_LABEL_COLUMN_NAME].value_counts().index)\nplt.ylabel('Count')\nfor p in risk_plot.patches:\n    height = p.get_height()\n    risk_plot.text(p.get_x()+p.get_width()/2., height + 1,'{0:.0%}'.format(height/float(len(df))),ha=\"center\") \nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We can get use frequency counts charts to get an understanding of the categorical features relative to Risk\n\n- We can see in the `CheckingStatus` visualization, loan applications with 'no_checking' have a higher occurence of Risk versus loans with other checking status values.\n- We can see in the `CreditHistory` visualization, the loans that have no credits (i.e. all credit has been paid back) have no occurences of Risk (at least in this dataset). There is a small count of Risk for those applicants that have paid back all credit to date. And there is a higher frequency or ratio of Risk for applicants that have existing credit (i.e outstanding credit).\n\n### NOTE: The creation of these plots can take several minutes"}, {"metadata": {}, "cell_type": "code", "source": "# Categorical feature count plots\nf, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10), (ax11, ax12), (ax13, ax14)) = plt.subplots(7, 2, figsize=(25, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14 ]\n\nfor i in range(len(categorical_features)):\n    sns.countplot(x = categorical_features[i], hue=TARGET_LABEL_COLUMN_NAME, data=df, ax=ax[i])\n    ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We can get use histrogram and boxplots to get an understanding of the distribution of our continuous / numerical features relative to Risk.\n\n- We can see that for loans that have Risk, the `InstallmentPercent` tends to be higher (i.e. the loans with Risk tend to have loan amounts with higher percentage of the loan applicants disposable income).\n- We can see that those with 'No Risk' seem to be those with fewer existing credit loans at the bank (`ExistingCreditCount`)\n"}, {"metadata": {}, "cell_type": "code", "source": "# Continuous feature histograms.\nf, ((ax1, ax2),(ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(25, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]\nfor i in range(len(continuous_features)):\n    #sns.distplot(df[continuous_features[i]], bins=20, color=\"blue\", hist=True, ax=ax[i])\n    sns.distplot(df[df.Risk == 'Risk'][continuous_features[i]], bins=20, color=\"Red\", hist=True, ax=ax[i])\n    sns.distplot(df[df.Risk == 'No Risk'][continuous_features[i]], bins=20, color=\"blue\", hist=True, ax=ax[i])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Plot boxplots of numerical columns. More variation in the boxplot implies higher significance. \nf, ((ax1, ax2),(ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(25, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]\nfor i in range(len(continuous_features)):\n    sns.boxplot(x = TARGET_LABEL_COLUMN_NAME, y = continuous_features[i], data=df, ax=ax[i])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3.0 Create a model\n\nNow we can create our machine learning model. You could use the insights / intuition gained from the data visualization steps above to what kind of model to create or which features to use. We will create a simple classification model."}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nimport pandas as pd\nimport json\n\nspark = SparkSession.builder.getOrCreate()\ndf_data = spark.createDataFrame(df)\ndf_data.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1 Split the data into training and test sets"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "spark_df = df_data\n(train_data, test_data) = spark_df.randomSplit([0.8, 0.2], 24)\n\nprint(\"Number of records for training: \" + str(train_data.count()))\nprint(\"Number of records for evaluation: \" + str(test_data.count()))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2 Examine the Spark DataFrame Schema\nLook at the data types to determine requirements for feature engineering"}, {"metadata": {}, "cell_type": "code", "source": "spark_df.printSchema()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.3 Use StringIndexer to encode a string column of labels to a column of label indices\n\nWe are using the Pipeline package to build the development steps as pipeline. \nWe are using StringIndexer to handle categorical / string features from the dataset. StringIndexer encodes a string column of labels to a column of label indices\n\nWe then use VectorAssembler to asemble these features into a vector. Pipelines API requires that input variables are passed in  a vector"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler, SQLTransformer\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model\n\n#Create StringIndexer columns whose names are same as the categorical column with an appended _IX.\ncategorical_num_features = [x + '_IX' for x in categorical_features]\nsi_list = [StringIndexer(inputCol=nm_in, outputCol=nm_out) for nm_in, nm_out in zip(categorical_features, categorical_num_features)]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Encode our target label column (i.e Risk or No Risk). \n# Also, creates an label convert which performs an inverse map to get back a 'Risk' or 'No Risk' label from the encoded prediction.\nsi_label = StringIndexer(inputCol=TARGET_LABEL_COLUMN_NAME, outputCol=\"label\").fit(spark_df)\nlabel_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_label.labels)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Construct all encoded categorical features plus continuous features into a vector\nva_features = VectorAssembler(inputCols=categorical_num_features + continuous_features, outputCol=\"features\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.4 Create a pipeline, and fit a model using RandomForestClassifier \nAssemble all the stages into a pipeline. We don't expect a clean linear regression, so we'll use RandomForestClassifier to find the best decision tree for the data.\n\nThe pipeline will consist of: the feature string indexing step, the label string indexing Step, vector sssembly of all features step, random forest classifier, label converter step, and ending with a feature filter step.\n\n**Note: If you want filter features from model output, you could use the feature filter by replacing `*` with feature names to be retained in SQLTransformer statement.**"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "classifier = RandomForestClassifier(featuresCol=\"features\")\nfeature_filter = SQLTransformer(statement=\"SELECT * FROM __THIS__\")\npipeline = Pipeline(stages= si_list + [si_label, va_features, classifier, label_converter, feature_filter])\n\nmodel = pipeline.fit(train_data)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "predictions = model.transform(test_data)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderROC')\narea_under_curve = evaluatorDT.evaluate(predictions)\n\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderPR')\narea_under_PR = evaluatorDT.evaluate(predictions)\n#default evaluation is areaUnderROC\nprint(\"areaUnderROC = %g\" % area_under_curve, \"areaUnderPR = %g\" % area_under_PR)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.5 evaluate more metrics by exporting them into pandas and numpy"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "from sklearn.metrics import classification_report\ny_pred = predictions.toPandas()['prediction']\nprint(y_pred)\ny_pred = ['Risk' if pred == 1.0 else 'No Risk' for pred in y_pred]\nprint(y_pred)\ny_test = test_data.toPandas()[TARGET_LABEL_COLUMN_NAME]\nprint(classification_report(y_test, y_pred, target_names=['Risk', 'No Risk']))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4.0 Create and export data set to Cognos Analytics\n\nWe will use our model to score some new credit applications.\nWe will then take the results of our model scoring, export it to Cognos Analytics where we can create some new visualizations."}, {"metadata": {}, "cell_type": "code", "source": "# Use our CA Data Connector to access the new credit applications stored in Cognos Analytics.\n# In order to access the data, modify the following line by changing the path and filename to match your data asset in CA.\n# NOTE: the file name should be similar to \"german_credit_new_apps_data.csv\".\ndf_new = CADataConnector.read_data(path=\".public_folders/__data_folder__/__data_asset__\")\ndf_new.head(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# create a Spark dataframe\ndf_data_new = spark.createDataFrame(df_new)\n\n# score the data using our model\npredictions = model.transform(df_data_new)\n\n# convert risk to string\ny_pred = predictions.toPandas()['prediction']\ny_pred = ['Risk' if pred == 1.0 else 'No Risk' for pred in y_pred]\nprint(y_pred)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# create a new data frame from our new credit applications data frame, but only keep the CustomerID column \ndf_new_scored = df_new\ndf_new_scored = df_new_scored.drop(columns=['CheckingStatus', 'LoanDuration', 'CreditHistory', 'LoanPurpose', 'LoanAmount', 'ExistingSavings', 'EmploymentDuration', 'InstallmentPercent', 'Sex', 'OthersOnLoan', 'CurrentResidenceDuration', 'OwnsProperty', 'Age', 'InstallmentPlans', 'Housing', 'ExistingCreditsCount', 'Job', 'Dependents', 'Telephone', 'ForeignWorker'], axis=1, errors='ignore')\n\n# now add in a new column named PREDICTED_RISK and assign the risks generated from the model\ndf_new_scored['PredictedRisk'] = y_pred\nprint(df_new_scored)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Write the new data frame back out to Cognos Analytics\n# Update the path to match your Cognos Analytics folder\nCADataConnector.write_data(df_new_scored, path=\".public_folders/__data_folder__/german_credit_new_apps_scored.csv\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Congratulations!\n\nThe new scored data should now be located in your Cognos Analytics data folder.\nReturn to your Cognos Analytics dashboard and start creating some visualizations."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}